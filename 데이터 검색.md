## 데이터 검색(Sql)
```
spark = SparkSession.builder.master('app').appName("sparkSQL").getOrCreate()

테이블_변수1 = spark.read.csv( '../상위폴더/파일_이름.csv'
    , header=True         # 헤더 O
    , inferSchema=True    # 스키마 O/X
    )

테이블_변수 = 테이블_변수1.createOrReplaceTempView('테이블_변수')

sql = 'SQL 코드'/'''SQL 코드'''    # 테이블 = 테이블_변수2
spark.sql(sql).show()
```
---

## 데이터 검색
+ ### 테이블 1개
    ```
    import pyspark.sql.functions as F       # 집계함수
    
    spark = SparkSession.builder.appName('이름').getOrCreate()
    
    테이블_변수
        .groupBy('속성_리스트')
        .filter(조건)
        
        .select('속성_리스트') / .agg( F.집계함수('속성_리스트') )    # 속성 / 집계함수
        .alias('별칭')    # 속성 / 집계함수 별칭
        
        .distinct()    # 중복X
        .orderBy('속성_리스트'
            , ascending = False    # 내림차순
        #   , ascending = True     # 오름차순
            )    
        
        .show(n)    # n행
    ```
    >정보
    >```
    >테이블_변수.show()
    >테이블_변수.column()    # 속성
    >테이블_변수.printSchema()    # 속성, 속성_타입
    >테이블_변수.summary( ['집계함수_리스트'] ).show()     # 집계함수    
    >```
    >ORDER BY
    >>1개 
    >>```
    >>테이블_변수.orderBy('속성'
    >>    , ascending = False    # 내림차순
    >>#   , ascending = True     # 오름차순
    >>    )
    >>```
    >>2개 이상
    >>```
    >>테이블_변수.orderBy( 
    >>    ['속성_리스트']
    >>    , ascending = [1/0, ..., 1/0]    # 1:오름차순, 0:내림차순 
    >>    )
    >>```
    >
    >GROUP BY
    >```
    >테이블_변수.groupBy('속성_리스트').agg( F.집계함수('속성_리스트')
    >```
    >
    >조건
    >```
    >테이블_변수.filter( (조건1) &/| ... &/| (조건n) )
    >```
    >>논리
    >>```
    >>between(a, b), &, |, ...
    >>```
    >>
    >>키워드
    >>```
    >>like('%문자열%')
    >>```
    >집계함수
    >```
    >COUNT, MAX, MIN, SUM, AVG
    >```

+ ### 테이블 2개
    >(공통)속성이 다른 경우
    >```
    >테이블_변수 = 테이블_변수1.join(
    >    테이블_변수2
    >    , 테이블_변수1.(공통)속성 == 테이블_변수2.(공통)속성
    >    , 'inner/outer'    # 내부/외부
    >    )     
    >```
    >
    >(공통)속성이 같은 경우
    >```
    >테이블_변수 = 테이블_변수1.join( 테이블_변수2, '(공통)속성', 'inner/outer' ) 
    >```
  
+ ### 테이블 n개
    ```
    위와 동일
    ```